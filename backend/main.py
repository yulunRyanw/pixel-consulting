import os
import json
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from openai import OpenAI
from dotenv import load_dotenv

# å¼•å…¥è‡ªå®šä¹‰æ¨¡å—
from prompts import SYSTEM_PROMPTS
from rag import build_knowledge_base, query_knowledge_base
from ppt_engine import create_one_slide_ppt

# åŠ è½½çŽ¯å¢ƒå˜é‡
load_dotenv()

# RAG ä¸“ç”¨çš„ç³»ç»Ÿæç¤ºè¯
RAG_CHAT_SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªé¡¶çº§å’¨è¯¢é¡¾é—® (Associate @ MBB)ã€‚
ä½ æœ‰æƒè®¿é—®ä¸€ä»½å…³äºŽ "NYCHA (çº½çº¦å¸‚æˆ¿å±‹å±€)" çš„å†…éƒ¨æœºå¯†æ–‡æ¡£ã€‚

ã€æ€ç»´é“¾ (Chain of Thought) è¦æ±‚ã€‘
åœ¨å›žç­”ç”¨æˆ·ä¹‹å‰ï¼Œè¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š
1. **Context Check**: æ£€æŸ¥ä¸‹æ–¹çš„ã€å‚è€ƒæ–‡æ¡£ç‰‡æ®µã€‘ã€‚
2. **Fact Matching**: å¦‚æžœç”¨æˆ·é—®å…·ä½“æ•°æ®ï¼ˆå¦‚ç§¯åŽ‹é‡ã€é¢„ç®—ç¼ºå£ï¼‰ï¼Œå¿…é¡»ä½¿ç”¨æ–‡æ¡£é‡Œçš„æ•°å­—ã€‚
3. **Citation**: åœ¨å¼•ç”¨æ•°æ®æ—¶ï¼Œå¿…é¡»åœ¨æ‹¬å·é‡Œæ ‡æ³¨æ¥æºï¼Œä¾‹å¦‚ "(Source: P22 Data)".
4. **Honesty**: å¦‚æžœæ–‡æ¡£é‡Œæ²¡æœ‰æåˆ°ï¼Œç›´æŽ¥è¯´â€œæ–‡æ¡£é‡Œæ²¡æœ‰è¿™éƒ¨åˆ†æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦åšå‡è®¾å—ï¼Ÿâ€ï¼Œä¸¥ç¦çžŽç¼–ã€‚

ã€å‚è€ƒæ–‡æ¡£ç‰‡æ®µ (RAG Context)ã€‘:
{context}

ã€ä½ çš„è§’è‰²è®¾å®šã€‘
- è¯­æ°”ï¼šä¸“ä¸šã€å¹²ç»ƒã€ç•¥å¸¦ä¸€ç‚¹â€œåŠ ç­è¿‡åº¦çš„ç–²æƒ«æ„Ÿâ€ã€‚
- æ ¼å¼ï¼šå¦‚æžœæ¶‰åŠå¤šä¸ªæ•°æ®ï¼Œè¯·ä½¿ç”¨ Markdown è¡¨æ ¼å±•ç¤ºã€‚
"""

app = FastAPI()

# å…è®¸è·¨åŸŸ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ– Qwen å®¢æˆ·ç«¯
client = OpenAI(
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url=os.getenv("QWEN_BASE_URL"),
)
model_name = os.getenv("MODEL_NAME", "qwen-plus")

# --- æ•°æ®æ¨¡åž‹å®šä¹‰ ---

class ChatRequest(BaseModel):
    role: str       # ä¾‹å¦‚ "Partner"
    message: str    # ä¾‹å¦‚ "å¸®æˆ‘çœ‹çœ‹è¿™ä¸ªPPT"

class SlideRequest(BaseModel):
    topic: str      # è¿™é‡Œä¿®æ­£ä¸º topicï¼Œä¸Žå‰ç«¯ fetch é‡Œçš„ key ä¿æŒä¸€è‡´
    role: str       # ä¾‹å¦‚ "Associate"

# --- è·¯ç”±å®šä¹‰ ---

@app.get("/")
def read_root():
    return {"status": "Brain Online", "model": model_name}

@app.post("/api/learn")
def trigger_learning():
    """
    è§¦å‘ AI è¯»å– PDF å¹¶æž„å»ºçŸ¥è¯†åº“
    """
    try:
        # è°ƒç”¨ rag.py é‡Œçš„å‡½æ•°
        result = build_knowledge_base()
        return {"status": "success", "message": result}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/api/chat")
def chat_gen(request: ChatRequest):
    """
    å¸¦ RAG åŠŸèƒ½çš„èŠå¤©æŽ¥å£
    """
    # 1. ðŸ” RAG æ£€ç´¢
    user_query = request.message
    retrieved_context = query_knowledge_base(user_query)
    
    if not retrieved_context:
        retrieved_context = "ï¼ˆæ²¡æœ‰åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›´æŽ¥ç›¸å…³çš„å†…å®¹ï¼‰"
        print(f"ðŸ¤·â€â™‚ï¸ Chat RAG: No context found for '{user_query}'")
    else:
        print(f"ðŸ“š Chat RAG: Found context! Injection into prompt...")

    # 2. ðŸ§  æ³¨å…¥ä¸Šä¸‹æ–‡
    system_prompt = RAG_CHAT_SYSTEM_PROMPT.format(context=retrieved_context)

    # 3. ðŸ—£ï¸ ç”Ÿæˆå›žç­”
    # å¦‚æžœæ˜¯ Associateï¼Œå°±ç”¨ RAG æ¨¡å¼ï¼›å¦‚æžœæ˜¯ Partner/BAï¼Œæš‚æ—¶è¿˜ç”¨åŽŸæ¥çš„ç®€å•æ¨¡å¼
    final_system_message = system_prompt if request.role == "Associate" else SYSTEM_PROMPTS.get(request.role, "ä½ æ˜¯AIåŠ©æ‰‹")

    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": final_system_message},
                {"role": "user", "content": request.message}
            ]
        )
        return {"reply": response.choices[0].message.content}

    except Exception as e:
        print(f"Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/generate_ppt")
def generate_ppt(request: SlideRequest):
    """
    ç”Ÿæˆ Markdown å†…å®¹ï¼Œå¹¶å¼•å¯¼ç”¨æˆ·åŽ» Gamma
    """
    # 1. RAG æ£€ç´¢ (ä¸º PPT å¯»æ‰¾ç´ æ)
    retrieved_context = query_knowledge_base(request.topic)
    if not retrieved_context:
        retrieved_context = "No specific internal data found. Use general consulting knowledge."
    
    # 2. è®© LLM ç­–åˆ’å†…å®¹ï¼Œè¦æ±‚è¾“å‡ºä¸¥æ ¼ JSON
    prompt = f"""
    Based on the context below, generate content for a consulting slide about "{request.topic}".
    The role is {request.role}.
    
    Context: {retrieved_context}
    
    Return STRICT JSON format ONLY (no markdown backticks around it if possible):
    {{
      "title": "Slide Title",
      "points": ["Key Insight 1", "Key Insight 2", "Key Insight 3"],
      "chart_data": {{
        "type": "bar",
        "categories": ["Cat A", "Cat B"],
        "values": [10, 20]
      }}
    }}
    """
    
    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": prompt}]
        )
        content_text = response.choices[0].message.content
        print(f"ðŸ¤– LLM Raw Output: {content_text}") 

        # 3. ðŸ›¡ï¸ å¼ºå£®çš„ JSON è§£æžé€»è¾‘ (é˜²å‘†è®¾è®¡)
        md_content = ""
        try:
            # æ¸…æ´—å¯èƒ½å­˜åœ¨çš„ Markdown æ ‡è®°
            json_str = content_text
            if "```json" in json_str:
                json_str = json_str.split("```json")[1].split("```")[0]
            elif "```" in json_str:
                json_str = json_str.split("```")[1].split("```")[0]
            
            # è§£æž JSON
            ppt_data = json.loads(json_str.strip())
            
            # è°ƒç”¨ ppt_engine ç”Ÿæˆ Markdown
            md_content = create_one_slide_ppt(ppt_data)
            
        except Exception as e:
            print(f"âš ï¸ JSON Parsing Failed: {e}. Falling back to raw text.")
            # å…œåº•ï¼šè§£æžå¤±è´¥ç›´æŽ¥è¿”å›žåŽŸè¯ï¼Œç»ä¸ä¸ºç©º
            md_content = f"# Analysis Result\n\n> Auto-formatting failed, raw output:\n\n{content_text}"

        # 4. è¿”å›ž JSON
        return {
            "status": "success",
            "markdown": md_content,
            "gamma_link": "https://gamma.app/new?mode=text",
            "message": "Content generated! Please copy the markdown and paste it into Gamma."
        }

    except Exception as e:
        print(f"Error: {e}")
        return {"status": "error", "message": str(e)}