import os
import pdfplumber
# ğŸ‘‡ ä¿®å¤äº†ä½ æˆªå›¾é‡Œçš„æŠ¥é”™ï¼Œä½¿ç”¨æ–°ç‰ˆå¼•ç”¨è·¯å¾„
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from dotenv import load_dotenv

load_dotenv()

embeddings = DashScopeEmbeddings(
    model="text-embedding-v1",
    dashscope_api_key=os.getenv("DASHSCOPE_API_KEY")
)

DB_PATH = "faiss_index"

def build_knowledge_base():
    """
    è¯»å–ç”¨æˆ·äººå·¥æ•´ç†çš„ doc.pdf (å…¨é‡è¯»å–ï¼Œå› ä¸ºå…¨æ˜¯ç²¾å)
    """
    pdf_path = "doc.pdf"
    
    if not os.path.exists(pdf_path):
        return "âŒ é”™è¯¯ï¼šbackend æ–‡ä»¶å¤¹é‡Œæ‰¾ä¸åˆ° doc.pdfï¼"

    print(f"ğŸ“– æ­£åœ¨è¯»å–äººå·¥ç²¾é€‰æ•™æ: {pdf_path} ...")
    
    selected_docs = []
    total_pages = 0
    
    try:
        with pdfplumber.open(pdf_path) as pdf:
            total_pages = len(pdf.pages)
            print(f"ğŸ“„ æ–‡ä»¶å…± {total_pages} é¡µï¼Œæ­£åœ¨é€é¡µæå–...")
            
            for i, page in enumerate(pdf.pages):
                # æå–æ–‡å­—
                text = page.extract_text()
                
                if not text:
                    text = "(æœ¬é¡µæ— æ–‡å­—)"
                else:
                    # ğŸ” ç®€å•çš„ Debug: çœ‹çœ‹æœ‰æ²¡æœ‰è¯»åˆ°é‚£ä¸ªå…³é”®æ•°å­—
                    if "330" in text or "330,000" in text or "Backlog" in text:
                        print(f"âœ¨ åœ¨ç¬¬ {i+1} é¡µå‘ç°äº† Backlog å…³é”®æ•°æ®ï¼")
                    if "16B" in text or "17B" in text:
                        print(f"ğŸ’° åœ¨ç¬¬ {i+1} é¡µå‘ç°äº† Capital Needs å…³é”®æ•°æ®ï¼")

                # å°è£…æˆ Document å¯¹è±¡
                doc = Document(
                    page_content=f"[Manual Ref: Page {i+1}]\n{text}",
                    metadata={"source": "manual_doc.pdf", "page": i+1}
                )
                selected_docs.append(doc)

    except Exception as e:
        return f"âŒ PDF è¯»å–å¤±è´¥: {str(e)}"

    if not selected_docs:
        return "âŒ æ²¡æœ‰æå–åˆ°ä»»ä½•å†…å®¹ï¼Œè¯·æ£€æŸ¥ PDF æ˜¯å¦ä¸ºç©ºã€‚"

    print(f"âœ… æˆåŠŸæå– {len(selected_docs)} é¡µã€‚æ­£åœ¨åˆ‡åˆ†å¹¶å­˜å…¥å¤§è„‘...")

    # åˆ‡åˆ†è®¾ç½®ï¼šå› ä¸ºä½ çš„æ•´ç†éå¸¸å¯†é›†ï¼Œæˆ‘ä»¬æŠŠå—ç¨å¾®è®¾å¤§ä¸€ç‚¹ï¼Œé‡å å¤šä¸€ç‚¹
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=300)
    texts = text_splitter.split_documents(selected_docs)
    
    try:
        # é‡å»ºç´¢å¼• (è¦†ç›–æ—§çš„)
        db = FAISS.from_documents(texts, embeddings)
        db.save_local(DB_PATH)
        print(f"ğŸ’¾ çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼å·²ä¿å­˜åˆ° '{DB_PATH}'ã€‚")
        return f"Success: Ingested {len(selected_docs)} pages of manual context. Ready for Level 2 consulting."
    except Exception as e:
        return f"âŒ å‘é‡åŒ–/ä¿å­˜å¤±è´¥: {str(e)}"

def query_knowledge_base(query):
    """
    æ£€ç´¢é€»è¾‘
    """
    if not os.path.exists(DB_PATH):
        return None

    try:
        db = FAISS.load_local(DB_PATH, embeddings, allow_dangerous_deserialization=True)
        # æœç´¢æœ€ç›¸å…³çš„ 4 ä¸ªç‰‡æ®µ
        docs = db.similarity_search(query, k=4)
        context = "\n\n".join([d.page_content for d in docs])
        return context
    except Exception as e:
        print(f"RAG Error: {e}")
        return None